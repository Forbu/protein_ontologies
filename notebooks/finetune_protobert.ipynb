{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this notebook, we will finetune the Protobert model on the dataset.\n",
    "We first have to create a dataset class that will be used to load the data.\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# bio  package\n",
    "from Bio import SeqIO\n",
    "\n",
    "# import lightning utils\n",
    "import lightning as L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for the protein sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, parquet_file_input, parquet_target=None) -> None:\n",
    "        \"\"\"\n",
    "        params:\n",
    "        - parquet_input: path to the parquet file containing the sequences\n",
    "        - target: path to the target file\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # we read the parquet file\n",
    "        self.sequences = pd.read_parquet(parquet_file_input)\n",
    "        self.target = pd.read_parquet(parquet_target) if parquet_target else None\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "\n",
    "        # we get the sequence\n",
    "        sequence = self.sequences.iloc[index].sequence\n",
    "\n",
    "\n",
    "        # we retrieve EntryID\n",
    "        entry_id = self.sequences.iloc[index].other_entry\n",
    "    \n",
    "        # we tokenize the sequence\n",
    "        tokenized_sequence = self.tokenizer(sequence, return_tensors='pt')\n",
    "\n",
    "        # scalar input\n",
    "        scalar_organism = self.sequences.iloc[index].organism  \n",
    "        animal = self.sequences.iloc[index].animal\n",
    "\n",
    "        # we get the target value if it exists\n",
    "        target = self.target.loc[entry_id].values if self.target is not None else None\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized_sequence,\n",
    "            'target': target,\n",
    "            'organism': scalar_organism,\n",
    "            'animal': animal\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_parquet_input = \"/workspaces/protein_ontologies/dataset_kg/Train/train_sequences.parquet\"\n",
    "path_parquet_target = \"/workspaces/protein_ontologies/dataset_kg/Train/train_labels.parquet\"\n",
    "\n",
    "# we create the dataset\n",
    "dataset = ProteinDataset(path_parquet_input, path_parquet_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2,  2, 21, 17, 10,  8, 15,  8, 10, 22,  6, 16, 20, 15, 11, 15, 20, 22,\n",
       "         14, 14, 24,  9, 16,  8, 21, 10, 18,  5,  8,  9, 19, 20, 17,  9,  8,  6,\n",
       "         10, 24,  5,  5, 13, 14,  9, 15, 10, 16, 11, 16, 14, 12, 19, 19, 11, 18,\n",
       "          5, 12, 18, 16,  5, 13, 17, 12, 13,  8, 23,  8, 23,  7, 11, 14, 16, 20,\n",
       "         16, 12, 14,  7, 15,  7,  8, 16, 19,  9, 10, 16, 17, 19, 15, 12, 12, 10,\n",
       "         11, 12,  9, 11,  6, 10, 10, 11, 10, 13,  5, 15,  7,  8, 11, 14, 20, 12,\n",
       "          7, 20, 17,  5, 17, 11, 11, 14,  7,  8, 11, 16, 24, 17, 20, 20,  5, 10,\n",
       "         23, 12,  5,  7,  9, 15, 12, 10, 22,  6, 11, 20, 24, 14, 12, 11, 10, 12,\n",
       "          5,  5,  5, 18, 22, 11, 15, 12, 22,  8, 10,  8,  5, 20, 23,  5,  7, 12,\n",
       "         15, 14, 19, 10, 17, 11, 13,  6, 12,  5,  9, 10, 16,  8, 15, 15, 11,  8,\n",
       "          7, 20, 22, 16,  6,  6, 13, 14, 13, 18, 19,  9, 12, 14, 13, 10, 19,  9,\n",
       "         11, 11, 17,  8,  5,  5,  9,  5, 14, 17, 12,  8, 16, 11, 17, 24,  6, 18,\n",
       "          7, 19, 11, 20,  3,  3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torch import nn\n",
    "\n",
    "class ProteinModel(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Module to finetune the Protobert model.\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_layer) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "    def compute_loss(self, batch: dict) -> Any:\n",
    "        \"\"\"\n",
    "        Compute the loss for a batch.\n",
    "        \"\"\"\n",
    "        # we get the input_ids\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "\n",
    "        # we get the output\n",
    "        output = self.model(input_ids)\n",
    "\n",
    "        # we get the target\n",
    "        target = batch[\"target\"]\n",
    "\n",
    "        # we compute the loss\n",
    "        loss = self.loss_fn(output, target)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, batch: dict) -> Any:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        return self.model(batch[\"input_ids\"])\n",
    "    \n",
    "    def training_step(self, batch: dict, batch_idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "        \"\"\"\n",
    "        loss = self.compute_loss(batch)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss\n",
    "        }\n",
    "    \n",
    "    def validation_step(self, batch: dict, batch_idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "        \"\"\"\n",
    "        loss = self.compute_loss(batch)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

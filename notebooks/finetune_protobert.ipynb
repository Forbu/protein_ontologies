{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this notebook, we will finetune the Protobert model on the dataset.\n",
    "We first have to create a dataset class that will be used to load the data.\n",
    "\"\"\"\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import lightning utils\n",
    "import lightning as L\n",
    "\n",
    "# init wandb logger\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for the protein sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, parquet_file_input, parquet_target=None) -> None:\n",
    "        \"\"\"\n",
    "        params:\n",
    "        - parquet_input: path to the parquet file containing the sequences\n",
    "        - target: path to the target file\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # we read the parquet file\n",
    "        self.sequences = pd.read_parquet(parquet_file_input)\n",
    "        self.target = pd.read_parquet(parquet_target) if parquet_target else None\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "\n",
    "        # we get the sequence\n",
    "        sequence = self.sequences.iloc[index].sequence\n",
    "\n",
    "\n",
    "        # we retrieve EntryID\n",
    "        entry_id = self.sequences.iloc[index].other_entry\n",
    "    \n",
    "        # we tokenize the sequence\n",
    "        tokenized_sequence = self.tokenizer(sequence, return_tensors='pt')\n",
    "\n",
    "        # we squeeze every tensor\n",
    "        tokenized_sequence = {key: value.squeeze() for key, value in tokenized_sequence.items()}\n",
    "\n",
    "        # scalar input\n",
    "        scalar_organism = self.sequences.iloc[index].organism  \n",
    "        animal = self.sequences.iloc[index].animal\n",
    "\n",
    "        # we get the target value if it exists\n",
    "        target = self.target.loc[entry_id].values if self.target is not None else None\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized_sequence,\n",
    "            'target': target,\n",
    "            'organism': scalar_organism,\n",
    "            'animal': animal\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_parquet_input = \"/workspaces/protein_ontologies/dataset_kg/Train/train_sequences.parquet\"\n",
    "path_parquet_target = \"/workspaces/protein_ontologies/dataset_kg/Train/train_labels.parquet\"\n",
    "\n",
    "# we create the dataset\n",
    "dataset = ProteinDataset(path_parquet_input, path_parquet_target)\n",
    "\n",
    "# we create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 2,  2, 21, 17, 10,  8, 15,  8, 10, 22,  6, 16, 20, 15, 11, 15, 20, 22,\n",
       "         14, 14, 24,  9, 16,  8, 21, 10, 18,  5,  8,  9, 19, 20, 17,  9,  8,  6,\n",
       "         10, 24,  5,  5, 13, 14,  9, 15, 10, 16, 11, 16, 14, 12, 19, 19, 11, 18,\n",
       "          5, 12, 18, 16,  5, 13, 17, 12, 13,  8, 23,  8, 23,  7, 11, 14, 16, 20,\n",
       "         16, 12, 14,  7, 15,  7,  8, 16, 19,  9, 10, 16, 17, 19, 15, 12, 12, 10,\n",
       "         11, 12,  9, 11,  6, 10, 10, 11, 10, 13,  5, 15,  7,  8, 11, 14, 20, 12,\n",
       "          7, 20, 17,  5, 17, 11, 11, 14,  7,  8, 11, 16, 24, 17, 20, 20,  5, 10,\n",
       "         23, 12,  5,  7,  9, 15, 12, 10, 22,  6, 11, 20, 24, 14, 12, 11, 10, 12,\n",
       "          5,  5,  5, 18, 22, 11, 15, 12, 22,  8, 10,  8,  5, 20, 23,  5,  7, 12,\n",
       "         15, 14, 19, 10, 17, 11, 13,  6, 12,  5,  9, 10, 16,  8, 15, 15, 11,  8,\n",
       "          7, 20, 22, 16,  6,  6, 13, 14, 13, 18, 19,  9, 12, 14, 13, 10, 19,  9,\n",
       "         11, 11, 17,  8,  5,  5,  9,  5, 14, 17, 12,  8, 16, 11, 17, 24,  6, 18,\n",
       "          7, 19, 11, 20,  3,  3]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2,  2, 21,  5,  6,  7, 13, 20, 23,  5, 15, 15, 22, 14,  6, 16, 15, 13,\n",
      "         22, 11,  9, 24,  5, 16,  5, 20, 19, 22, 10, 13, 22, 20,  5,  5, 10, 19,\n",
      "         19,  5, 19, 10, 19,  5, 19, 19, 19, 16, 23,  8,  8, 23,  8, 23, 18, 15,\n",
      "          9, 12,  7, 17, 12,  6, 16,  6, 21,  5, 13,  8, 10, 13,  5, 10,  5,  6,\n",
      "         11,  7, 15, 20, 10,  5, 19, 21, 12,  9, 18, 12, 17, 17, 16,  6,  5, 12,\n",
      "          7,  5, 16,  8,  6, 12, 13,  7, 18,  6, 15,  6, 12,  5, 20, 13,  9,  5,\n",
      "         10,  8, 15,  9, 13,  9,  9,  5,  6, 12, 13,  6, 12,  6,  6, 16, 10,  6,\n",
      "         15, 13, 12, 12, 10,  8, 16, 12, 15, 12,  7, 12,  9, 12,  8, 15,  7,  7,\n",
      "          7,  7, 12, 13, 12,  6, 10,  9, 20, 15,  9, 19,  8, 12, 10, 17, 11, 10,\n",
      "         12, 20, 10, 17,  5, 16, 18, 13,  9, 13, 21, 15,  6,  8,  6, 12,  5, 24,\n",
      "         12, 18, 18, 12, 18, 21, 13, 12,  3,  3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "torch.Size([1, 190])\n",
      "torch.Size([1, 500])\n"
     ]
    }
   ],
   "source": [
    "# we test the dataloader\n",
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"])\n",
    "    print(batch[\"target\"])\n",
    "    # look at the shape of the input_ids\n",
    "    print(batch[\"input_ids\"][\"input_ids\"].shape)\n",
    "    # look at the shape of the target\n",
    "    print(batch[\"target\"].shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torch import nn\n",
    "import transformers\n",
    "\n",
    "global_model = \"Rostlab/prot_bert\"\n",
    "NB_OUTPUT = 500\n",
    "\n",
    "class ProteinModel(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Module to finetune the Protobert model.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = transformers.AutoConfig.from_pretrained(global_model)\n",
    "        self.config.update({'output_hidden_states':True})\n",
    "\n",
    "        self.model = BertForMaskedLM.from_pretrained(global_model, config=self.config)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size * 2, NB_OUTPUT),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "\n",
    "    def compute_loss(self, batch: dict) -> Any:\n",
    "        \"\"\"\n",
    "        Compute the loss for a batch.\n",
    "        \"\"\"\n",
    "        # we get the output\n",
    "        output = self(batch)\n",
    "\n",
    "        # we get the target\n",
    "        target = batch[\"target\"]\n",
    "\n",
    "        # we compute the loss\n",
    "        loss = self.loss_fn(output, target.float())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, batch: dict) -> Any:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        hidden_states = self.model(**batch[\"input_ids\"])[\"hidden_states\"]\n",
    "        \n",
    "        # concat the 2 hidden states\n",
    "        hidden_states = torch.cat([hidden_states[-1], hidden_states[-2]], dim=-1)\n",
    "\n",
    "        # average the hidden states\n",
    "        hidden_states = torch.mean(hidden_states, dim=1)\n",
    "\n",
    "        # we pass it through the head\n",
    "        hidden_states = self.head(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "    \n",
    "    def training_step(self, batch: dict, batch_idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "        \"\"\"\n",
    "        loss = self.compute_loss(batch)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss\n",
    "        }\n",
    "    \n",
    "    def validation_step(self, batch: dict, batch_idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "        \"\"\"\n",
    "        loss = self.compute_loss(batch)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss\n",
    "        }\n",
    "    \n",
    "    def configure_optimizers(self) -> Any:\n",
    "        \"\"\"\n",
    "        Configure the optimizer.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-6)\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# we initialize the model\n",
    "model = ProteinModel()\n",
    "\n",
    "# test the model\n",
    "with torch.no_grad():\n",
    "    output = model(batch)\n",
    "\n",
    "    loss = model.compute_loss(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# init wandb\n",
    "wandb.init(project=\"protein-ontologies\", name=\"protobert\")\n",
    "\n",
    "logger = L.WandbLogger(project=\"protein-ontologies\", name=\"protobert\", entity='forbu14')\n",
    "\n",
    "# init the trainer\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    accumulate_grad_batches=8,\n",
    "    max_time=\"00:01:00:00\",\n",
    "    logger=logger,\n",
    "    gradient_clip_val=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6905)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
